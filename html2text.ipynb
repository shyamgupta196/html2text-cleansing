{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HTML Information Extraction Toolkit**\n",
    "#### Introduction\n",
    "\n",
    "This notebook demonstrates how to preprocess HTML pages and extract useful textual information. It uses two Python libraries: \n",
    "- `trafilatura` for general-purpose text extraction.\n",
    "- `news-please` for structured information extraction, especially for news articles.\n",
    "\n",
    "We will walk through the process step-by-step, extracting clean and meaningful text from raw HTML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Required Libraries**\n",
    "Make sure you have the necessary libraries installed. If not, run the following command in your terminal or notebook:\n",
    "\n",
    "```bash\n",
    "!pip install trafilatura news-please\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trafilatura\n",
      "  Using cached trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting news-please\n",
      "  Using cached news_please-1.6.15-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from trafilatura) (2025.4.26)\n",
      "Collecting charset_normalizer>=3.4.0 (from trafilatura)\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting courlan>=1.3.2 (from trafilatura)\n",
      "  Using cached courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting htmldate>=1.9.2 (from trafilatura)\n",
      "  Using cached htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.1 (from trafilatura)\n",
      "  Using cached justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting lxml>=5.3.0 (from trafilatura)\n",
      "  Downloading lxml-6.0.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from trafilatura) (1.26.20)\n",
      "Collecting Scrapy>=1.1.0 (from news-please)\n",
      "  Using cached scrapy-2.13.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting PyMySQL>=0.7.9 (from news-please)\n",
      "  Using cached PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting psycopg2-binary>=2.8.4 (from news-please)\n",
      "  Downloading psycopg2-binary-2.9.10.tar.gz (385 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson>=1.5.8 (from news-please)\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting elasticsearch>=2.4 (from news-please)\n",
      "  Using cached elasticsearch-9.0.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.3.2 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from news-please) (4.12.3)\n",
      "Collecting readability-lxml>=0.6.2 (from news-please)\n",
      "  Using cached readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langdetect>=1.0.7 (from news-please)\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from news-please) (2.9.0.post0)\n",
      "Collecting plac>=0.9.6 (from news-please)\n",
      "  Using cached plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dotmap>=1.2.17 (from news-please)\n",
      "  Using cached dotmap-1.3.30-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from news-please)\n",
      "  Using cached PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting warcio>=1.3.3 (from news-please)\n",
      "  Using cached warcio-1.7.5-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting ago>=0.0.9 (from news-please)\n",
      "  Using cached ago-0.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from news-please) (1.17.0)\n",
      "Collecting hurry.filesize>=0.9 (from news-please)\n",
      "  Using cached hurry.filesize-0.9.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bs4 (from news-please)\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting faust-cchardet>=2.1.18 (from news-please)\n",
      "  Downloading faust_cchardet-2.1.19-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from news-please) (1.37.3)\n",
      "Collecting redis (from news-please)\n",
      "  Using cached redis-6.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting newspaper4k>=0.9.3.1 (from news-please)\n",
      "  Using cached newspaper4k-0.9.3.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting lxml-html-clean>=0.1.1 (from news-please)\n",
      "  Using cached lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from news-please) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from beautifulsoup4>=4.3.2->news-please) (2.5)\n",
      "Requirement already satisfied: babel>=2.16.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from courlan>=1.3.2->trafilatura) (2.16.0)\n",
      "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
      "  Using cached tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch>=2.4->news-please)\n",
      "  Using cached elastic_transport-8.17.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
      "  Using cached dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting lxml>=5.3.0 (from trafilatura)\n",
      "  Downloading lxml-5.4.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from hurry.filesize>=0.9->news-please) (63.4.1)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (11.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (6.0.2)\n",
      "Collecting feedparser>=6.0.0 (from newspaper4k>=0.9.3.1->news-please)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: nltk>=3.6.6 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.4 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from newspaper4k>=0.9.3.1->news-please) (2.32.3)\n",
      "Collecting tldextract>=2.0.1 (from newspaper4k>=0.9.3.1->news-please)\n",
      "  Using cached tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting chardet (from readability-lxml>=0.6.2->news-please)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting cssselect (from readability-lxml>=0.6.2->news-please)\n",
      "  Using cached cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from Scrapy>=1.1.0->news-please) (44.0.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from Scrapy>=1.1.0->news-please) (0.7.1)\n",
      "Collecting itemadapter>=0.1.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting itemloaders>=1.0.1 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from Scrapy>=1.1.0->news-please) (24.2)\n",
      "Collecting parsel>=1.5.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting protego>=0.1.15 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached protego-0.5.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyopenssl>=22.0.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from Scrapy>=1.1.0->news-please) (25.0.0)\n",
      "Collecting queuelib>=1.4.2 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting service-identity>=18.1.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting twisted>=21.7.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached twisted-25.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting w3lib>=1.17.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Using cached w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting zope-interface>=5.1.0 (from Scrapy>=1.1.0->news-please)\n",
      "  Downloading zope.interface-7.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.3 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from boto3->news-please) (1.37.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from boto3->news-please) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from boto3->news-please) (0.11.3)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from redis->news-please) (5.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from cryptography>=37.0.0->Scrapy>=1.1.0->news-please) (1.17.1)\n",
      "Requirement already satisfied: pytz>=2024.2 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3)\n",
      "Collecting sgmllib3k (from feedparser>=6.0.0->newspaper4k>=0.9.3.1->news-please)\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from nltk>=3.6.6->newspaper4k>=0.9.3.1->news-please) (4.67.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from pandas>=1.4->newspaper4k>=0.9.3.1->news-please) (2025.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from requests>=2.26.0->newspaper4k>=0.9.3.1->news-please) (3.7)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (24.3.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper4k>=0.9.3.1->news-please)\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper4k>=0.9.3.1->news-please) (3.13.1)\n",
      "Collecting automat>=24.8.0 (from twisted>=21.7.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from twisted>=21.7.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from twisted>=21.7.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from twisted>=21.7.0->Scrapy>=1.1.0->news-please)\n",
      "  Using cached incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=37.0.0->Scrapy>=1.1.0->news-please) (2.21)\n",
      "Requirement already satisfied: tomli in /opt/anaconda3/envs/uni/lib/python3.9/site-packages (from incremental>=24.7.0->twisted>=21.7.0->Scrapy>=1.1.0->news-please) (2.0.1)\n",
      "Using cached trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
      "Using cached news_please-1.6.15-py3-none-any.whl (96 kB)\n",
      "Using cached ago-0.1.0-py3-none-any.whl (6.5 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "Using cached courlan-1.3.2-py3-none-any.whl (33 kB)\n",
      "Using cached dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
      "Using cached elasticsearch-9.0.2-py3-none-any.whl (914 kB)\n",
      "Downloading faust_cchardet-2.1.19-cp39-cp39-macosx_11_0_arm64.whl (135 kB)\n",
      "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Using cached htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
      "Using cached justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
      "Downloading lxml-5.4.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached newspaper4k-0.9.3.1-py3-none-any.whl (296 kB)\n",
      "Using cached plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
      "Using cached PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Using cached PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "Using cached readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
      "Using cached scrapy-2.13.3-py3-none-any.whl (321 kB)\n",
      "Using cached warcio-1.7.5-py2.py3-none-any.whl (40 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached redis-6.2.0-py3-none-any.whl (278 kB)\n",
      "Using cached cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Using cached dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
      "Using cached elastic_transport-8.17.1-py3-none-any.whl (64 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
      "Using cached parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
      "Using cached protego-0.5.0-py3-none-any.whl (10 kB)\n",
      "Using cached queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
      "Using cached service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
      "Using cached tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached twisted-25.5.0-py3-none-any.whl (3.2 MB)\n",
      "Using cached w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-7.2-cp39-cp39-macosx_11_0_arm64.whl (208 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached automat-25.4.16-py3-none-any.whl (42 kB)\n",
      "Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Building wheels for collected packages: hurry.filesize, langdetect, psycopg2-binary, sgmllib3k\n",
      "  Building wheel for hurry.filesize (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hurry.filesize: filename=hurry.filesize-0.9-py3-none-any.whl size=4116 sha256=5e2b9c12af619f677ac707c4116d281a5b6e70e44b9dba43ed6808c76592cbda\n",
      "  Stored in directory: /Users/shyam/Library/Caches/pip/wheels/3c/97/5e/2475af1d4343e1d41becdfa497e764625ac3b226ac9299aeda\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=f08f22dd617f7dda1e13060cdeccb0974ad9e419aae1149ad6adf30983ae3318\n",
      "  Stored in directory: /Users/shyam/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for psycopg2-binary (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2-binary: filename=psycopg2_binary-2.9.10-cp39-cp39-macosx_11_0_arm64.whl size=132299 sha256=7df5b0eaf7115def265325c263edf6206564dccf59ac15219f8d7630b71fa5f4\n",
      "  Stored in directory: /Users/shyam/Library/Caches/pip/wheels/b3/78/4b/26baded4713ddbdca47cb9dcdab88aae8371bdcda44f9e07eb\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=f986397f3d4aeb8e57a0eaaf97b30026a49b39ececba2723e66daed142409df6\n",
      "  Stored in directory: /Users/shyam/Library/Caches/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built hurry.filesize langdetect psycopg2-binary sgmllib3k\n",
      "Installing collected packages: sgmllib3k, PyDispatcher, plac, hjson, faust-cchardet, dotmap, ago, zope-interface, warcio, w3lib, tld, redis, queuelib, PyMySQL, pyasn1, psycopg2-binary, protego, lxml, langdetect, itemadapter, incremental, hyperlink, hurry.filesize, feedparser, elastic-transport, cssselect, constantly, charset_normalizer, chardet, automat, twisted, pyasn1-modules, parsel, lxml-html-clean, elasticsearch, dateparser, courlan, bs4, service-identity, requests-file, itemloaders, htmldate, tldextract, readability-lxml, justext, trafilatura, Scrapy, newspaper4k, news-please\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyDispatcher-2.0.7 PyMySQL-1.1.1 Scrapy-2.13.3 ago-0.1.0 automat-25.4.16 bs4-0.0.2 chardet-5.2.0 charset_normalizer-3.4.2 constantly-23.10.4 courlan-1.3.2 cssselect-1.3.0 dateparser-1.2.2 dotmap-1.3.30 elastic-transport-8.17.1 elasticsearch-9.0.2 faust-cchardet-2.1.19 feedparser-6.0.11 hjson-3.1.0 htmldate-1.9.3 hurry.filesize-0.9 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 justext-3.0.2 langdetect-1.0.9 lxml-5.4.0 lxml-html-clean-0.4.2 news-please-1.6.15 newspaper4k-0.9.3.1 parsel-1.10.0 plac-1.4.5 protego-0.5.0 psycopg2-binary-2.9.10 pyasn1-0.6.1 pyasn1-modules-0.4.2 queuelib-1.8.0 readability-lxml-0.8.4.1 redis-6.2.0 requests-file-2.1.0 service-identity-24.2.0 sgmllib3k-1.0.0 tld-0.13.1 tldextract-5.3.0 trafilatura-2.0.0 twisted-25.5.0 w3lib-2.3.1 warcio-1.7.5 zope-interface-7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install trafilatura news-please\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import Necessary Libraries**\n",
    "\n",
    "Here, we import the required libraries to handle file operations, multiprocessing for efficiency, and text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "import re\n",
    "import trafilatura\n",
    "from newsplease import NewsPlease\n",
    "from Trafilatura import *\n",
    "from NewsPlease import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Set the Directory Path**\n",
    "\n",
    "\n",
    "Define the base directory where all your HTML files are stored. Make sure you have a folder named `html` containing the files you want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URI = 'html/'  # Update this path if your folder is elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Create a Function to Clean Text**\n",
    "\n",
    "\n",
    "The `clean_text` function removes unnecessary whitespace, redundant hyphens, and formatting inconsistencies to produce clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by removing extra whitespace, unnecessary hyphens, etc.\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    cleaned_text = cleaned_text.strip()       # Strip leading and trailing whitespace\n",
    "    cleaned_text = cleaned_text.replace(\"- \", \"\")  # Remove hyphens followed by spaces\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Create a Function to Extract Text Using Trafilatura**\n",
    "\n",
    "\n",
    "The `extract_text_trafilatura` function processes an HTML file to extract its main content and metadata using Trafilatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(filename):\n",
    "    json_object = {}\n",
    "    print(filename)\n",
    "    with open(BASE_URI + filename, 'r', encoding='utf-8') as file:\n",
    "        try:\n",
    "            html_content = file.read()\n",
    "            result = trafilatura.extract(html_content, no_fallback=True, include_links=False, include_comments=False,\n",
    "                                         include_tables=False, include_images=False, include_formatting=True)\n",
    "            metadata = trafilatura.extract_metadata(html_content)\n",
    "            json_object['title'] = metadata.title\n",
    "            json_object['main_text'] = clean_text(result)\n",
    "            json_object['filename'] = filename\n",
    "        except Exception as e:\n",
    "            print(\"Error processing\", filename)\n",
    "            print(\"Exception:\", str(e))\n",
    "\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Create a Function to Extract Text Using NewsPlease**\n",
    "\n",
    "The `extract_text_newsplease` function processes an HTML file to extract structured news information using NewsPlease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def news_it(filename):\n",
    "#     \"\"\"\n",
    "#     Extracts news information from HTML files and creates a JSON object.\n",
    "\n",
    "#     Args:\n",
    "#         filename (str): The name of the HTML file to process.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing extracted news information or an empty dictionary if extraction fails.\n",
    "#     \"\"\"\n",
    "#     json_object = {}  # Initialize an empty dictionary to store news information\n",
    "#     print(filename)\n",
    "\n",
    "#     # Open and read the HTML file\n",
    "#     with open(BASE_URI + filename, 'r', encoding='utf-8') as file:\n",
    "#         try:\n",
    "#             # Use NewsPlease to parse the HTML content and extract news information\n",
    "#             news = NewsPlease.from_html(file.read())\n",
    "#             json_object['title'] = news.title\n",
    "#             json_object['description'] = news.description\n",
    "#             json_object['main_text'] = news.maintext\n",
    "#             json_object['language'] = news.language\n",
    "#             json_object['filename'] = filename\n",
    "#         except Exception as e:\n",
    "#             print(\"Error processing\", filename)\n",
    "#             print(\"Exception:\", str(e))\n",
    "\n",
    "#     return json_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Process Files Using Multiprocessing**\n",
    "\n",
    "We use Python's `multiprocessing` to process multiple HTML files in parallel for efficiency. This block processes all files in the `html` folder using both `Trafilatura` and `NewsPlease`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-46:\n",
      "Process SpawnPoolWorker-43:\n",
      "Process SpawnPoolWorker-44:\n",
      "Process SpawnPoolWorker-42:\n",
      "Process SpawnPoolWorker-41:\n",
      "Process SpawnPoolWorker-45:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-52:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-55:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-57:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-56:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-58:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-59:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-60:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-61:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-62:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-63:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-64:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-65:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-66:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-67:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-68:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-69:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-70:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-71:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-72:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-73:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-74:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-75:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-76:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-77:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-78:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-80:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'extract_text' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a multiprocessing pool for the extract_text function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(os\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Process the list of HTML files using the extract_text function in parallel\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     extract_results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Write the extracted text information to a JSON file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml_json_trafilatura.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/uni/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/uni/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/envs/uni/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a multiprocessing pool for the extract_text function\n",
    "with multiprocessing.Pool(os.cpu_count()) as pool:\n",
    "    # Process the list of HTML files using the extract_text function in parallel\n",
    "    extract_results = pool.map(extract_text, os.listdir(BASE_URI))\n",
    "    # Write the extracted text information to a JSON file\n",
    "    with open('html_json_trafilatura.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(extract_results, f, ensure_ascii=False, indent=4)\n",
    "    print(\"JSON data written to 'html_json_trafilatura.json'\")\n",
    "\n",
    "# Create a separate multiprocessing pool for the news_it function\n",
    "with multiprocessing.Pool(os.cpu_count()) as pool:\n",
    "    # Process the list of HTML files using the news_it function in parallel\n",
    "    news_results = pool.map(news_it, os.listdir(BASE_URI))\n",
    "    # Write the extracted news information to a JSON file\n",
    "    with open('html_json_news.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(news_results, f, ensure_ascii=False, indent=4)\n",
    "    print(\"JSON data written to 'html_json_news.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Notes**\n",
    "\n",
    "This notebook demonstrates how to extract clean and structured text from HTML files using two methods. The results are saved as JSON files:\n",
    "\n",
    "1. `html_json_trafilatura.json`: Output from Trafilatura.\n",
    "2. `html_json_news.json`: Output from NewsPlease.\n",
    "\n",
    "You can analyze these JSON files further for your research or application needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
